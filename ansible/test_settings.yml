# Debug mode (Will print out extra debug information. Only enable if asked
# to by Red Hat)
redhat_debug_mode: True

# Red Hat subscription username, password, pool_id used for accessing Red
# Hat repositories.  If you do not have one please ask your Red Hat EPM
# to assist.
rh_sub_mode: false
rh_sub_username:
rh_sub_pass:
rh_sub_pool_id:

# Enable the below if internally testing for QE and use the command found
# at -> https://wiki.test.redhat.com/ctrautma and search for the section
# Subscription manager testing and enclose it with ""
# DO NOT COMMIT THIS FILE TO GITHUB if this change is made!!!!!
qe_subscription_mode: false
qe_subscription_command:

# Alternate solution to subscription manager and QE subscription. Allows
# custom repos to be used instead.
custom_repo_setup: true
custom_repo_path: "http://9.114.224.56/~drc/ftp3.repo"

# T-Rex version to use (example v2.53 : Note please include the v).
# This is the version that will be downloaded from git and installed on the
# T-Rex server.
trex_path: https://trex-tgn.cisco.com/trex/release
trex_version: v2.72

# T-Rex server NIC names (example eth1, enp68s0f0, etc...)
trex_interface_1: ens4f0
trex_interface_2: ens4f1
trex_interface_1_pciid: "0000:58:00.0"
trex_interface_2_pciid: "0000:58:00.1"

# Openvswitch package location. While you can pull this from the fast datapath
# repository it is recommended to contact your development representative to
# get the latest version. Then copy this to the DUT and specify its location.
ovs_rpm_path: "http://9.114.224.56/~drc/openvswitch2.11-2.11.0-27.el8.ppc64le.rpm"

# Openvswitch selinux package location. While you can pull this from the fast
# datapath repository it is recommended to contact your development
# representative to get the latest version.  Then copy this to the DUT and
# specify its location.
ovs_selinux_rpm_path: "http://9.114.224.56/~drc/openvswitch-selinux-extra-policy-1.0-19.el8.noarch.rpm"

# VM isolated core list (x86_64 should use 1,2,3 and Power should use 1,2,3,4,5)
# ToDo: Move this get_cpulist.sh
vm_isolated_cpus: 1,2,3,4,5

# DUT NIC Device names and PCI addresses
#DRC - [ltc19-u30] MLX5 (NUMA node 0)
#dut_interface_1_pciid: "0000:01:00.0"
#dut_interface_2_pciid: "0000:01:00.1"

#DRC - [ltc19-u30] i40e (NUMA node 8)
dut_interface_1_pciid: "0034:01:00.0"
dut_interface_2_pciid: "0034:01:00.1"

# VM NIC interface names and PCI addresses.  Note that interface naming is
# influenced by the underlying emulated machine type.  x86 systems will
# typically use enp[1-X]f[0-X] names while Power systems will typically
# use eth[0-X].
# ToDo: Try and eliminate the dependency on interface names in favor of
#       PCI addresses only.  Could use:
#       sudo lshw -businfo -class network | grep 0034:01:00.0 | awk '{print $2}'
vm_interface_1_pciid: "0000:00:01.0"
vm_interface_2_pciid: "0000:00:02.0"

# ToDo: Need description here
tc_offload_card_type: "Unknown"

# User mode driver. For most devices this will be "vfio-pci", but some NICs
# such as Mellanox ConnectX-5 don't require user access to hardware registers,
# so the driver should be "none".  And in some cases other drivers such as
# "uio_pci_generic" driver may be required, though not officially supported
# by Red Hat.
# dut_driver: none
# dut_driver: vfio-pci
# dut_driver: uio_pci_generic
dut_driver: vfio-pci
vm_driver: uio_pci_generic
trex_driver: vfio-pci

# Path to RHEL guest image for PVP tests.  Please download the image from
# the Red Hat website.  See README for more info.
# This is the location where the file is located on the DUT.
rhel_guest_image_path: /home/vm/images/rhel-guest-image-8.1-263.ppc64le.qcow2

# CPU model info, needs to be modified to match your DUT CPU type. If an error
# occurs with virt-install, might need to modify this line to a model present
# in /usr/share/libvirt/cpu_map.xml or 'virsh capabilities'
#dut_cpu_model: SandyBridge
#dut_cpu_model: Haswell-noTSX
#dut_cpu_model: Skylake-Server-IBRS
#dut_cpu_model: POWER9
dut_cpu_model: host

# Hugepage kernel command-line options for the DUT and T-Rex systems.
# The VM is configured with 8GB of RAM, OVS/DPDK is configured with
# 4G per NUMA node, and the tests expect at least two NUMA nodes, so
# 32GB is the recommended setting for both systems.
dut_hugepages: 64
dut_hugepagesz: 1G
dut_default_hugepagesz: 1G

trex_hugepages: 32
trex_hugepagesz: 1G
trex_default_hugepagesz: 1G

# iommu kernel command-line option. x86 based systems typically require "pt",
# Power based systems should select "none".  
dut_iommu_mode: "none"
trex_iommu_mode: "pt"

# intel_iommu kernel command-line option. x86 based systems will require "on",
# Power based systems ignore this setting
dut_intel_iommu_mode: "none"
trex_intel_iommu_mode: "on"
